{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4 - 20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mashruravi/clf-tiny-imagenet/blob/master/A4_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWdOExpR516Q",
        "colab_type": "text"
      },
      "source": [
        "# EIP 3.0 - Assignment 4\n",
        "*Submission by Ravi Suresh Mashru*\n",
        "\n",
        "\n",
        "## Abstract\n",
        "This assignment required training a deep neural network to perform image classification on the Tiny ImageNet dataset. The assignment contraints were:\n",
        "1. At least 5 variants of image augmentation should be performed.\n",
        "1. A validation score of >45% has to be obtained.\n",
        "1. The network should be trained for up to 500 epochs only.\n",
        "1. The deep network should not have more than 26 million parameters.\n",
        "1. 1x1 convolutions cannot be used to increase the number of channels.\n",
        "1. Dropout layers cannot be used.\n",
        "1. Fully connected layers cannot be used.\n",
        "\n",
        "\n",
        "In my submission below,\n",
        "1. I use the following variants of image augmentation:\n",
        "  * Rotation\n",
        "  * Horizontal Shift\n",
        "  * Vertical Shift\n",
        "  * Horizontal Flip\n",
        "  * Vertical Flip\n",
        "  \n",
        "2. I obtained a validation score of **55.28%**.\n",
        "3. I obtained the above score by training the network for only **60 epochs**.\n",
        "4. I used a network with approximately **9.2 million parameters**, which is over 16 million parameters less than the constraint.\n",
        "5. I used the **cylic learning rate** scheme to train the neural network.\n",
        "6. I did not use 1x1 convolutions, dropout, or fully connected layers.\n",
        "\n",
        "I also used the TPU environment on Google Colaboratory to train my network, which allowed me to complete one epoch in approximately **100 seconds **(compared to 700 seconds using a GPU, with the same architecture).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEqDiUS14cn",
        "colab_type": "code",
        "outputId": "686b68f1-a822-4cf0-8a39-a56811bb8a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Download the dataset\n",
        "!curl http://cs231n.stanford.edu/tiny-imagenet-200.zip --output tiny-imagenet-200.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  236M  100  236M    0     0  4905k      0  0:00:49  0:00:49 --:--:-- 8000k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ3WNjQzFZRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the data\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N91lWm4LWQXF",
        "colab_type": "code",
        "outputId": "b1b2cb35-0f08-47b0-900f-90696ce03079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load training and validation data\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "num_classes = 200\n",
        "imgs_per_class = 500\n",
        "\n",
        "X_train = np.zeros([num_classes * imgs_per_class, 64, 64, 3], dtype='uint8')\n",
        "y_train = np.zeros([num_classes * imgs_per_class, 1], dtype='uint8')\n",
        "\n",
        "# Create class to integer map\n",
        "class_int_map = {}\n",
        "class_list = []\n",
        "for i, class_name in enumerate(os.listdir(os.getcwd() + '/tiny-imagenet-200/train')):\n",
        "    if not class_name.startswith('.'): # Ignore the .DS_Store folder on Mac\n",
        "        class_list.append(class_name)\n",
        "        \n",
        "for num, class_name in enumerate(class_list):\n",
        "    class_int_map[class_name] = num\n",
        "\n",
        "i=0\n",
        "for class_name, class_index in class_int_map.items():\n",
        "    \n",
        "    for img_name in os.listdir(os.getcwd() + '/tiny-imagenet-200/train/' + class_name + '/images'):\n",
        "        y_train[i] = [class_int_map[class_name]]\n",
        "        \n",
        "        img = Image.open(os.getcwd() + '/tiny-imagenet-200/train/'+class_name+'/images/'+img_name)\n",
        "        X = np.array(img.resize((64, 64)))\n",
        "        \n",
        "        if(len(X.shape) == 2):\n",
        "            X = np.array([X, X, X])\n",
        "            X = np.transpose(X, (1, 2, 0))\n",
        "        X_train[i] = X\n",
        "        \n",
        "        i+=1\n",
        "        \n",
        "end = time.time()\n",
        "print('Training data loaded in {} seconds...'.format(end - start))\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "val_imgname_class_map = {}\n",
        "with open('tiny-imagenet-200/val/val_annotations.txt', 'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for line in lines:\n",
        "        if line: # to skip the last blank line\n",
        "            line_parts = line.split('\\t')\n",
        "            val_imgname_class_map[line_parts[0]] = line_parts[1]\n",
        "            \n",
        "val_img_names = os.listdir(os.getcwd() + '/tiny-imagenet-200/val/images/')\n",
        "val_img_count = len(val_img_names)\n",
        "\n",
        "X_val = np.zeros([val_img_count, 64, 64, 3], dtype='uint8')\n",
        "y_val = np.zeros([val_img_count, 1], dtype='uint8')\n",
        "\n",
        "i = 0\n",
        "for img_name in val_img_names:\n",
        "    y_val[i] = [class_int_map[val_imgname_class_map[img_name]]]\n",
        "    \n",
        "    img = Image.open(os.getcwd() + '/tiny-imagenet-200/val/images/'+img_name)\n",
        "    X = np.array(img.resize((64, 64)))\n",
        "    if(len(X.shape) == 2):\n",
        "        X = np.array([X, X, X])\n",
        "        X = np.transpose(X, (1, 2, 0))\n",
        "    X_val[i] = X\n",
        "    \n",
        "    i+=1\n",
        "    \n",
        "end = time.time()\n",
        "print('Validation data loaded in {} seconds...'.format(end - start))\n",
        "\n",
        "y_train_onehot = np_utils.to_categorical(y_train)\n",
        "y_val_onehot = np_utils.to_categorical(y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training data loaded in 27.454161882400513 seconds...\n",
            "Validation data loaded in 2.777902364730835 seconds...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snZIYr-6-VKT",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "For this assignment, I used a modified version of the Wide Residual Network (WRN) architecture. [Zagorukyo & Komodakis](https://arxiv.org/abs/1605.07146) show that deep residual networks are very slow to train because of diminishing feature reuse. They propose the WRN architecture which modifies residual blocks that were introduced in the ResNet architecture, by increasing their width (increasing the number of filters per layer) and decreasing their depth (reducing the number of ResNet blocks).\n",
        "\n",
        "I did not follow the exact architecture proposed in the paper. I tweaked some parts of the network based on learnings from EIP.\n",
        "\n",
        "In particular, I increased the number of channels in the first convolution layer to ensure that the inital layers of the network are learning as many features as possible.\n",
        "\n",
        "I also removed the fully connected layer and replaced it with a Global Average Pooling layer.\n",
        "\n",
        "I also did not use Dropout layers, and instead used L2 regularization and image augmentation to tackle overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6zlN9Mt3bUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Dropout\n",
        ")\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "weight_decay = 0.0005\n",
        "\n",
        "def initial_conv(input):\n",
        "    x = Conv2D(100, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_init\")(input)\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN_init\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(idx, init, base, k, strides=(1, 1)):\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_expand_\"+str(idx)+\"_1\")(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN_expand\"+str(idx))(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_expand_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    skip = Conv2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_expand_\"+str(idx)+\"_3\")(init)\n",
        "\n",
        "    m = add([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(idx, input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN1_\"+str(idx)+\"_1\")(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv1_\"+str(idx)+\"_1\")(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(rate=dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN1_\"+str(idx)+\"_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv1_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    m = add([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(idx, input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN2_\"+str(idx)+\"_1\")(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv2_\"+str(idx)+\"_1\")(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(rate=dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN2_\"+str(idx)+\"_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False,name=\"conv2_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    m = add([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(idx, input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN3_\"+str(idx)+\"_1\")(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv3_\"+str(idx)+\"_1\")(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(rate=dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN3_\"+str(idx)+\"_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv3_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    m = add([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(1, x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(i, x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN_1\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(2, x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(i, x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(3, x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(i, x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN_3\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(nb_classes, (1,1), padding=\"same\", name=\"conv_final\")(x)\n",
        "    x = GlobalAveragePooling2D(name=\"GAP\")(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzOBmV69OwHf",
        "colab_type": "code",
        "outputId": "e4cddb90-9f37-4b31-8901-df9bdb79936d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "model64 = create_wide_residual_network((64, 64, 3), nb_classes=200, N=4, k=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Wide Residual Network-28-5 created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B01qzOtg4AW",
        "colab_type": "code",
        "outputId": "6af5f0f9-d111-4f35-f4e3-b084af7b3a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3536
        }
      },
      "source": [
        "model64.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_init (Conv2D)              (None, 64, 64, 100)  2700        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "BN_init (BatchNormalizationV1)  (None, 64, 64, 100)  400         conv_init[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 64, 64, 100)  0           BN_init[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_1_1 (Conv2D)        (None, 64, 64, 80)   72000       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "BN_expand1 (BatchNormalizationV (None, 64, 64, 80)   320         conv_expand_1_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 64, 64, 80)   0           BN_expand1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_1_2 (Conv2D)        (None, 64, 64, 80)   57600       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_1_3 (Conv2D)        (None, 64, 64, 80)   8000        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 64, 64, 80)   0           conv_expand_1_2[0][0]            \n",
            "                                                                 conv_expand_1_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN1_0_1 (BatchNormalizationV1)  (None, 64, 64, 80)   320         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64, 64, 80)   0           BN1_0_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_0_1 (Conv2D)              (None, 64, 64, 80)   57600       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "BN1_0_2 (BatchNormalizationV1)  (None, 64, 64, 80)   320         conv1_0_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 80)   0           BN1_0_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_0_2 (Conv2D)              (None, 64, 64, 80)   57600       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 80)   0           add[0][0]                        \n",
            "                                                                 conv1_0_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN1_1_1 (BatchNormalizationV1)  (None, 64, 64, 80)   320         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 64, 64, 80)   0           BN1_1_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_1_1 (Conv2D)              (None, 64, 64, 80)   57600       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "BN1_1_2 (BatchNormalizationV1)  (None, 64, 64, 80)   320         conv1_1_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64, 64, 80)   0           BN1_1_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_1_2 (Conv2D)              (None, 64, 64, 80)   57600       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 64, 80)   0           add_1[0][0]                      \n",
            "                                                                 conv1_1_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN1_2_1 (BatchNormalizationV1)  (None, 64, 64, 80)   320         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 64, 64, 80)   0           BN1_2_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_2_1 (Conv2D)              (None, 64, 64, 80)   57600       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "BN1_2_2 (BatchNormalizationV1)  (None, 64, 64, 80)   320         conv1_2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 64, 64, 80)   0           BN1_2_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_2_2 (Conv2D)              (None, 64, 64, 80)   57600       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 64, 64, 80)   0           add_2[0][0]                      \n",
            "                                                                 conv1_2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN_1 (BatchNormalizationV1)     (None, 64, 64, 80)   320         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 64, 64, 80)   0           BN_1[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_2_1 (Conv2D)        (None, 32, 32, 160)  115200      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "BN_expand2 (BatchNormalizationV (None, 32, 32, 160)  640         conv_expand_2_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 160)  0           BN_expand2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_2_2 (Conv2D)        (None, 32, 32, 160)  230400      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_2_3 (Conv2D)        (None, 32, 32, 160)  12800       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32, 32, 160)  0           conv_expand_2_2[0][0]            \n",
            "                                                                 conv_expand_2_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN2_0_1 (BatchNormalizationV1)  (None, 32, 32, 160)  640         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 160)  0           BN2_0_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_0_1 (Conv2D)              (None, 32, 32, 160)  230400      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN2_0_2 (BatchNormalizationV1)  (None, 32, 32, 160)  640         conv2_0_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 160)  0           BN2_0_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_0_2 (Conv2D)              (None, 32, 32, 160)  230400      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 32, 32, 160)  0           add_4[0][0]                      \n",
            "                                                                 conv2_0_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN2_1_1 (BatchNormalizationV1)  (None, 32, 32, 160)  640         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 160)  0           BN2_1_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1 (Conv2D)              (None, 32, 32, 160)  230400      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN2_1_2 (BatchNormalizationV1)  (None, 32, 32, 160)  640         conv2_1_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 160)  0           BN2_1_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_2 (Conv2D)              (None, 32, 32, 160)  230400      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 32, 32, 160)  0           add_5[0][0]                      \n",
            "                                                                 conv2_1_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN2_2_1 (BatchNormalizationV1)  (None, 32, 32, 160)  640         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 160)  0           BN2_2_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1 (Conv2D)              (None, 32, 32, 160)  230400      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN2_2_2 (BatchNormalizationV1)  (None, 32, 32, 160)  640         conv2_2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 160)  0           BN2_2_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_2 (Conv2D)              (None, 32, 32, 160)  230400      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 32, 32, 160)  0           add_6[0][0]                      \n",
            "                                                                 conv2_2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN_2 (BatchNormalizationV1)     (None, 32, 32, 160)  640         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 160)  0           BN_2[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_3_1 (Conv2D)        (None, 16, 16, 320)  460800      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN_expand3 (BatchNormalizationV (None, 16, 16, 320)  1280        conv_expand_3_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 320)  0           BN_expand3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_3_2 (Conv2D)        (None, 16, 16, 320)  921600      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_expand_3_3 (Conv2D)        (None, 16, 16, 320)  51200       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 320)  0           conv_expand_3_2[0][0]            \n",
            "                                                                 conv_expand_3_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN3_0_1 (BatchNormalizationV1)  (None, 16, 16, 320)  1280        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 320)  0           BN3_0_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_0_1 (Conv2D)              (None, 16, 16, 320)  921600      activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN3_0_2 (BatchNormalizationV1)  (None, 16, 16, 320)  1280        conv3_0_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 320)  0           BN3_0_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_0_2 (Conv2D)              (None, 16, 16, 320)  921600      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 320)  0           add_8[0][0]                      \n",
            "                                                                 conv3_0_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN3_1_1 (BatchNormalizationV1)  (None, 16, 16, 320)  1280        add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 320)  0           BN3_1_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1 (Conv2D)              (None, 16, 16, 320)  921600      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN3_1_2 (BatchNormalizationV1)  (None, 16, 16, 320)  1280        conv3_1_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 320)  0           BN3_1_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_2 (Conv2D)              (None, 16, 16, 320)  921600      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 320)  0           add_9[0][0]                      \n",
            "                                                                 conv3_1_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN3_2_1 (BatchNormalizationV1)  (None, 16, 16, 320)  1280        add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 320)  0           BN3_2_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1 (Conv2D)              (None, 16, 16, 320)  921600      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BN3_2_2 (BatchNormalizationV1)  (None, 16, 16, 320)  1280        conv3_2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 320)  0           BN3_2_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_2 (Conv2D)              (None, 16, 16, 320)  921600      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 16, 320)  0           add_10[0][0]                     \n",
            "                                                                 conv3_2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BN_3 (BatchNormalizationV1)     (None, 16, 16, 320)  1280        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 320)  0           BN_3[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "conv_final (Conv2D)             (None, 16, 16, 200)  64200       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "GAP (GlobalAveragePooling2D)    (None, 200)          0           conv_final[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 200)          0           GAP[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 9,272,420\n",
            "Trainable params: 9,263,260\n",
            "Non-trainable params: 9,160\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BCedPT5xW0z",
        "colab_type": "code",
        "outputId": "5be0bfe1-f211-456f-f161-b83ceddbcfc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import os\n",
        "\n",
        "tpu_model64 = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model64,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model64.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.94.145.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11793125802818997041)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 222014773708665408)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 688473507330456592)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2562041439673439253)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6290636814627968912)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16232182569698177708)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14927012216668382101)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16584706200702275444)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13269519095236788859)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5473292238157884139)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9490772936611654457)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYafjqmZ_07r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image augmentation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "train_datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDSUVoTVBBst",
        "colab_type": "text"
      },
      "source": [
        "## Cyclic Learning Rate\n",
        "\n",
        "I used the cyclic learning rate scheme introduced by Leslie Smith in the paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186). In this scheme, the learning rate is continuously varied between two values during training instead of keeping it fixed.\n",
        "\n",
        "To get the two values between which the learning rate should be varied, a Learning Rate range test is performed. In this test, the learning rate is increased from an extremely low value to an extremely high value. The learning rate is updated for every batch sent to the neural network. The loss values of the network are then plotted against the learning rate on a graph. The learning rate on the graph _after the section_* where the reduction in loss is the steepest is kept as the upper bound of the cylic learning rate scheme. The lower bound is taken to be about 1/10 of this value.\n",
        "\n",
        "`*` In the paper, Leslie Smith recommends using the learning rate where the change in loss is the steepest as the upper bound of the cyclic learning rate scheme. However, in the [fast.ai lessons](https://course.fast.ai/), Jeremy Howard shows that it is actually better to use a learning rate that is slightly higher than that value as the upper bound."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxImbDQNN6eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or \n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```    \n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO8Yq3SSN7Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clr = CyclicLR(base_lr=0.00001, max_lr=1.5, step_size=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cXG0Jo4JngP",
        "colab_type": "code",
        "outputId": "53957caf-b62a-452a-a47c-bc06dea167d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2162
        }
      },
      "source": [
        "BATCH_SIZE=1024\n",
        "history_init = tpu_model64.fit(\n",
        "    X_train, y_train_onehot, batch_size=BATCH_SIZE,\n",
        "    epochs=50,\n",
        "    callbacks=[clr]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(128, 200), dtype=tf.float32, name='activation_25_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 9.999999747378752e-06, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7fafb40d98d0> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 30.648479223251343 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 9.999999747378752e-06 {1e-05}\n",
            "INFO:tensorflow:CPU -> TPU momentum: 0.0 {0.0}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "  2048/100000 [..............................] - ETA: 1:02:13 - loss: 10.4363 - acc: 0.0039WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.497647). Check your callbacks.\n",
            " 98304/100000 [============================>.] - ETA: 2s - loss: 10.4310 - acc: 0.0043INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(84,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(84, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(84, 200), dtype=tf.float32, name='activation_25_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7fafb40d98d0> [<tf.Variable 'tpu_140392624599560/SGD/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad68e630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad68e908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad68ef28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad635da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad5c8d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad58c550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad570898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad4fa518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad489e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad44f358>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad42d550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad3e09e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad328e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad383d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad2d99b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad2a0e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad1ea7b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad68e4e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad178e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad1d4d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad0aa9b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad073e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad0cfb00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad0029b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacfcce80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafad026da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacefe9b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacf20588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacec6ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faface519e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faface1de80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacdf9d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacd4e9b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafaccf4b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafaccbb6a0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafaccbb2e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacbef2b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacc11ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacb7dcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacb48518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacaadac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafacb0cf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafaca3e2b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafaca64ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafaca095f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac99aac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac95d668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac95d2b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac88fdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac836ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac85a128>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac76aac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac731668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac7312b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac6e6dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac686ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac62d5f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac5bbac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac5833c8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac547ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac4effd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac4806d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac446940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac446320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac3d6940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac39aa20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac340fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac2d06d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac2979b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac238ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac1c76d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac16ca20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac191fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac1236d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac068940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafac068320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafabff8940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafabfbea20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafabfe4fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7fafabef36d8>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 24.50210213661194 secs\n",
            "100000/100000 [==============================] - 195s 2ms/sample - loss: 10.4308 - acc: 0.0043\n",
            "Epoch 2/50\n",
            "100000/100000 [==============================] - 74s 742us/sample - loss: 10.4012 - acc: 0.0045\n",
            "Epoch 3/50\n",
            "100000/100000 [==============================] - 73s 733us/sample - loss: 10.3420 - acc: 0.0058\n",
            "Epoch 4/50\n",
            "100000/100000 [==============================] - 73s 730us/sample - loss: 10.2531 - acc: 0.0103\n",
            "Epoch 5/50\n",
            "100000/100000 [==============================] - 73s 731us/sample - loss: 10.1328 - acc: 0.0151\n",
            "Epoch 6/50\n",
            "100000/100000 [==============================] - 74s 737us/sample - loss: 9.9760 - acc: 0.0165\n",
            "Epoch 7/50\n",
            "100000/100000 [==============================] - 73s 732us/sample - loss: 9.7712 - acc: 0.0204\n",
            "Epoch 8/50\n",
            "100000/100000 [==============================] - 74s 738us/sample - loss: 9.5095 - acc: 0.0251\n",
            "Epoch 9/50\n",
            "100000/100000 [==============================] - 74s 735us/sample - loss: 9.1911 - acc: 0.0345\n",
            "Epoch 10/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 8.8205 - acc: 0.0459\n",
            "Epoch 11/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 8.4037 - acc: 0.0624\n",
            "Epoch 12/50\n",
            "100000/100000 [==============================] - 74s 737us/sample - loss: 7.9573 - acc: 0.0805\n",
            "Epoch 13/50\n",
            "100000/100000 [==============================] - 73s 730us/sample - loss: 7.5306 - acc: 0.1024\n",
            "Epoch 14/50\n",
            "100000/100000 [==============================] - 73s 733us/sample - loss: 7.1108 - acc: 0.1253\n",
            "Epoch 15/50\n",
            "100000/100000 [==============================] - 73s 731us/sample - loss: 6.6981 - acc: 0.1518\n",
            "Epoch 16/50\n",
            "100000/100000 [==============================] - 73s 735us/sample - loss: 6.3002 - acc: 0.1764\n",
            "Epoch 17/50\n",
            "100000/100000 [==============================] - 74s 736us/sample - loss: 5.9111 - acc: 0.2058\n",
            "Epoch 18/50\n",
            "100000/100000 [==============================] - 73s 732us/sample - loss: 5.5467 - acc: 0.2322\n",
            "Epoch 19/50\n",
            "100000/100000 [==============================] - 74s 735us/sample - loss: 5.2115 - acc: 0.2592\n",
            "Epoch 20/50\n",
            "100000/100000 [==============================] - 74s 738us/sample - loss: 4.8709 - acc: 0.2886\n",
            "Epoch 21/50\n",
            "100000/100000 [==============================] - 74s 736us/sample - loss: 4.5863 - acc: 0.3105\n",
            "Epoch 22/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 4.2933 - acc: 0.3377\n",
            "Epoch 23/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 4.0297 - acc: 0.3623\n",
            "Epoch 24/50\n",
            "100000/100000 [==============================] - 74s 739us/sample - loss: 3.8130 - acc: 0.3829\n",
            "Epoch 25/50\n",
            "100000/100000 [==============================] - 74s 735us/sample - loss: 3.5871 - acc: 0.4053\n",
            "Epoch 26/50\n",
            "100000/100000 [==============================] - 74s 738us/sample - loss: 3.4114 - acc: 0.4248\n",
            "Epoch 27/50\n",
            "100000/100000 [==============================] - 74s 737us/sample - loss: 3.2595 - acc: 0.4355\n",
            "Epoch 28/50\n",
            "100000/100000 [==============================] - 73s 731us/sample - loss: 3.0985 - acc: 0.4563\n",
            "Epoch 29/50\n",
            "100000/100000 [==============================] - 74s 736us/sample - loss: 2.9888 - acc: 0.4681\n",
            "Epoch 30/50\n",
            "100000/100000 [==============================] - 74s 735us/sample - loss: 2.8567 - acc: 0.4870\n",
            "Epoch 31/50\n",
            "100000/100000 [==============================] - 74s 738us/sample - loss: 2.7790 - acc: 0.4965\n",
            "Epoch 32/50\n",
            "100000/100000 [==============================] - 73s 729us/sample - loss: 2.6949 - acc: 0.5074\n",
            "Epoch 33/50\n",
            "100000/100000 [==============================] - 73s 731us/sample - loss: 2.6301 - acc: 0.5179\n",
            "Epoch 34/50\n",
            "100000/100000 [==============================] - 74s 739us/sample - loss: 2.5584 - acc: 0.5334\n",
            "Epoch 35/50\n",
            "100000/100000 [==============================] - 74s 736us/sample - loss: 2.5252 - acc: 0.5392\n",
            "Epoch 36/50\n",
            "100000/100000 [==============================] - 74s 735us/sample - loss: 2.4895 - acc: 0.5480\n",
            "Epoch 37/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 2.4608 - acc: 0.5556\n",
            "Epoch 38/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 2.4278 - acc: 0.5655\n",
            "Epoch 39/50\n",
            "100000/100000 [==============================] - 73s 734us/sample - loss: 2.4202 - acc: 0.5706\n",
            "Epoch 40/50\n",
            "100000/100000 [==============================] - 74s 736us/sample - loss: 2.4123 - acc: 0.5755\n",
            "Epoch 41/50\n",
            "100000/100000 [==============================] - 73s 732us/sample - loss: 2.3873 - acc: 0.5860\n",
            "Epoch 42/50\n",
            "100000/100000 [==============================] - 73s 731us/sample - loss: 2.3793 - acc: 0.5928\n",
            "Epoch 43/50\n",
            "100000/100000 [==============================] - 73s 735us/sample - loss: 2.3852 - acc: 0.5952\n",
            "Epoch 44/50\n",
            "100000/100000 [==============================] - 73s 735us/sample - loss: 2.3905 - acc: 0.6027\n",
            "Epoch 45/50\n",
            "100000/100000 [==============================] - 74s 736us/sample - loss: 2.3928 - acc: 0.6071\n",
            "Epoch 46/50\n",
            "100000/100000 [==============================] - 73s 735us/sample - loss: 2.4158 - acc: 0.6071\n",
            "Epoch 47/50\n",
            "100000/100000 [==============================] - 73s 729us/sample - loss: 2.3971 - acc: 0.6169\n",
            "Epoch 48/50\n",
            "100000/100000 [==============================] - 73s 733us/sample - loss: 2.4194 - acc: 0.6168\n",
            "Epoch 49/50\n",
            "100000/100000 [==============================] - 73s 733us/sample - loss: 2.4348 - acc: 0.6186\n",
            "Epoch 50/50\n",
            "100000/100000 [==============================] - 74s 737us/sample - loss: 2.4672 - acc: 0.6193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crFlKaw0LJTO",
        "colab_type": "code",
        "outputId": "97d83dc7-fc89-443a-aea9-3b0093a9bdd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        }
      },
      "source": [
        "lr = clr.history['lr']\n",
        "loss = clr.history['loss']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print('LR range vs iteration')\n",
        "plt.plot(range(len(lr)), lr)\n",
        "plt.show()\n",
        "print('Loss vs LR')\n",
        "plt.plot(lr, loss)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR range vs iteration\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ5iIiEgiDGEiIiKJMISJiIgk8v8BFbrRNL9iajQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss vs LR\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGJjOFLREQksv8HdxEuzdfeA8UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Op1nScDN1Rc",
        "colab_type": "text"
      },
      "source": [
        "From the above graph, the loss is reducing fastest when the learning rate is around 0.6. Therefore, as described above, we take the upper bound of the cyclic learning rate scheme as **0.8** and the lower bound as 1/10 of this value, i.e. **0.08**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UdmaHhFGmgh",
        "colab_type": "code",
        "outputId": "d2315cf2-ca47-42ab-b011-4ad6bc02a799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "import os\n",
        "\n",
        "model = create_wide_residual_network((64, 64, 3), nb_classes=200, N=4, k=5)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "clr = CyclicLR(base_lr=0.08, max_lr=0.8, step_size=980)\n",
        "# Step size = 980 => 20 epochs = 1 cycle, 10 epochs = 1 step size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wide Residual Network-28-5 created.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.94.145.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11793125802818997041)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 222014773708665408)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 688473507330456592)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2562041439673439253)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6290636814627968912)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16232182569698177708)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14927012216668382101)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16584706200702275444)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13269519095236788859)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5473292238157884139)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9490772936611654457)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPneoigAHBmk",
        "colab_type": "code",
        "outputId": "1bd7ff70-181d-4020-a437-05ba8dfb5430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3845
        }
      },
      "source": [
        "BATCH_SIZE=1024\n",
        "history1 = tpu_model.fit_generator(\n",
        "    train_datagen.flow(X_train, y_train_onehot, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=len(X_train)/BATCH_SIZE,\n",
        "    epochs=60,\n",
        "    callbacks=[clr],\n",
        "    validation_data=(X_val, y_val_onehot)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 64, 64, 3), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(128, 200), dtype=tf.float32, name='activation_51_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.07999999821186066, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7faf9e0a42b0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 45.5510573387146 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.07999999821186066 {0.08}\n",
            "INFO:tensorflow:CPU -> TPU momentum: 0.0 {0.0}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            " 1/98 [..............................] - ETA: 3:38:55 - loss: 10.4399 - acc: 0.0029WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (4.304830). Check your callbacks.\n",
            "97/98 [============================>.] - ETA: 2s - loss: 10.3871 - acc: 0.0062INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(84,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(84, 64, 64, 3), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(84, 200), dtype=tf.float32, name='activation_51_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7faf9e0a42b0> [<tf.Variable 'tpu_140392288837528/SGD/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97ba7550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97ba7828>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97ba7e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97bd09b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97b3af98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97aa4da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97b023c8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97a8ec18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97a1a208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf979dbd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf979bca20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97946d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf978ad4e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf978d34e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97897e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9781fd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97805a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9778dd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf976f54e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf977174e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf976dee80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf975e7d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9764aa20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf975d6d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9753f4e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9755ecc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf975026d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97431d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97494a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9741ed68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97362860>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97386e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf972ece80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97279a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9723e550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9729b4e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf971a7518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97170e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97131da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf970c4d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97083550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf97067898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96ff1518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96fb8e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96f7a9e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96f0bd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96ecb550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96eaf898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96e38518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96dffe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96dc3da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96d539e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96d13550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96cf7898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96c7c518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96bee080>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96c0ca90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96b98d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96b3e2e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96aa2e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96afe630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96a529b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96a16e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf969f4b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf969229e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf968eae80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96948d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf968989e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9685ee48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9683d588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf967689e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96732e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9678e5f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf966df9b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96626e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf96684b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf965b39e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf9657ae80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf965d4d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7faf964a89b0>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 41.7454195022583 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(84,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(84, 64, 64, 3), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(84, 200), dtype=tf.float32, name='activation_51_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.07999999821186066, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7faf8d931fd0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 35.70718598365784 secs\n",
            " 9408/10000 [===========================>..] - ETA: 3s - loss: 10.3189 - acc: 0.0135INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(74,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(74, 64, 64, 3), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(74, 200), dtype=tf.float32, name='activation_51_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7faf8d931fd0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 27.224350452423096 secs\n",
            "10000/10000 [==============================] - 85s 8ms/sample - loss: 10.3189 - acc: 0.0135\n",
            "98/98 [==============================] - 384s 4s/step - loss: 10.3864 - acc: 0.0062 - val_loss: 10.3189 - val_acc: 0.0135\n",
            "Epoch 2/60\n",
            "97/98 [============================>.] - ETA: 0s - loss: 10.2293 - acc: 0.0144INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(128, 64, 64, 3), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(128, 200), dtype=tf.float32, name='activation_51_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7faf8d931fd0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 20.607630491256714 secs\n",
            " 9216/10000 [==========================>...] - ETA: 2s - loss: 10.1159 - acc: 0.0164INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(98,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(98, 64, 64, 3), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(98, 200), dtype=tf.float32, name='activation_51_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7faf8d931fd0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 24.34547448158264 secs\n",
            "10000/10000 [==============================] - 57s 6ms/sample - loss: 10.1159 - acc: 0.0162\n",
            "98/98 [==============================] - 146s 1s/step - loss: 10.2282 - acc: 0.0144 - val_loss: 10.1159 - val_acc: 0.0162\n",
            "Epoch 3/60\n",
            "10000/10000 [==============================] - 4s 364us/sample - loss: 9.8056 - acc: 0.0177\n",
            "98/98 [==============================] - 92s 936ms/step - loss: 9.9796 - acc: 0.0155 - val_loss: 9.8057 - val_acc: 0.0177\n",
            "Epoch 4/60\n",
            "10000/10000 [==============================] - 4s 368us/sample - loss: 9.4083 - acc: 0.0278\n",
            "98/98 [==============================] - 99s 1s/step - loss: 9.6220 - acc: 0.0233 - val_loss: 9.4084 - val_acc: 0.0278\n",
            "Epoch 5/60\n",
            "10000/10000 [==============================] - 4s 363us/sample - loss: 8.9539 - acc: 0.0366\n",
            "98/98 [==============================] - 101s 1s/step - loss: 9.1719 - acc: 0.0340 - val_loss: 8.9536 - val_acc: 0.0366\n",
            "Epoch 6/60\n",
            "10000/10000 [==============================] - 4s 368us/sample - loss: 8.8562 - acc: 0.0268\n",
            "98/98 [==============================] - 99s 1s/step - loss: 8.6469 - acc: 0.0479 - val_loss: 8.8552 - val_acc: 0.0268\n",
            "Epoch 7/60\n",
            "10000/10000 [==============================] - 4s 358us/sample - loss: 8.2581 - acc: 0.0362\n",
            "98/98 [==============================] - 99s 1s/step - loss: 8.0613 - acc: 0.0663 - val_loss: 8.2571 - val_acc: 0.0362\n",
            "Epoch 8/60\n",
            "10000/10000 [==============================] - 4s 375us/sample - loss: 7.8522 - acc: 0.0470\n",
            "98/98 [==============================] - 100s 1s/step - loss: 7.4547 - acc: 0.0906 - val_loss: 7.8506 - val_acc: 0.0470\n",
            "Epoch 9/60\n",
            "10000/10000 [==============================] - 4s 369us/sample - loss: 7.3903 - acc: 0.0739\n",
            "98/98 [==============================] - 99s 1s/step - loss: 6.8821 - acc: 0.1136 - val_loss: 7.3885 - val_acc: 0.0739\n",
            "Epoch 10/60\n",
            "10000/10000 [==============================] - 4s 371us/sample - loss: 6.5331 - acc: 0.1108\n",
            "98/98 [==============================] - 99s 1s/step - loss: 6.3559 - acc: 0.1390 - val_loss: 6.5308 - val_acc: 0.1108\n",
            "Epoch 11/60\n",
            "10000/10000 [==============================] - 4s 369us/sample - loss: 6.0600 - acc: 0.1331\n",
            "98/98 [==============================] - 99s 1s/step - loss: 5.8231 - acc: 0.1706 - val_loss: 6.0589 - val_acc: 0.1331\n",
            "Epoch 12/60\n",
            "10000/10000 [==============================] - 4s 370us/sample - loss: 6.1709 - acc: 0.1053\n",
            "98/98 [==============================] - 99s 1s/step - loss: 5.3675 - acc: 0.2018 - val_loss: 6.1698 - val_acc: 0.1053\n",
            "Epoch 13/60\n",
            "10000/10000 [==============================] - 4s 359us/sample - loss: 5.1800 - acc: 0.1922\n",
            "98/98 [==============================] - 99s 1s/step - loss: 4.9766 - acc: 0.2361 - val_loss: 5.1807 - val_acc: 0.1922\n",
            "Epoch 14/60\n",
            "10000/10000 [==============================] - 4s 430us/sample - loss: 5.1681 - acc: 0.1765\n",
            "98/98 [==============================] - 100s 1s/step - loss: 4.6415 - acc: 0.2678 - val_loss: 5.1668 - val_acc: 0.1765\n",
            "Epoch 15/60\n",
            "10000/10000 [==============================] - 4s 391us/sample - loss: 5.4095 - acc: 0.1485\n",
            "98/98 [==============================] - 104s 1s/step - loss: 4.3663 - acc: 0.2960 - val_loss: 5.4104 - val_acc: 0.1485\n",
            "Epoch 16/60\n",
            "10000/10000 [==============================] - 4s 365us/sample - loss: 4.5858 - acc: 0.2444\n",
            "98/98 [==============================] - 98s 1s/step - loss: 4.1163 - acc: 0.3268 - val_loss: 4.5860 - val_acc: 0.2444\n",
            "Epoch 17/60\n",
            "10000/10000 [==============================] - 4s 369us/sample - loss: 4.2141 - acc: 0.2938\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.8955 - acc: 0.3542 - val_loss: 4.2139 - val_acc: 0.2938\n",
            "Epoch 18/60\n",
            "10000/10000 [==============================] - 4s 363us/sample - loss: 4.1079 - acc: 0.3024\n",
            "98/98 [==============================] - 98s 1s/step - loss: 3.6982 - acc: 0.3835 - val_loss: 4.1079 - val_acc: 0.3024\n",
            "Epoch 19/60\n",
            "10000/10000 [==============================] - 4s 369us/sample - loss: 3.9355 - acc: 0.3293\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.5312 - acc: 0.4106 - val_loss: 3.9362 - val_acc: 0.3293\n",
            "Epoch 20/60\n",
            "10000/10000 [==============================] - 4s 356us/sample - loss: 3.8390 - acc: 0.3425\n",
            "98/98 [==============================] - 100s 1s/step - loss: 3.3872 - acc: 0.4369 - val_loss: 3.8392 - val_acc: 0.3425\n",
            "Epoch 21/60\n",
            "10000/10000 [==============================] - 4s 353us/sample - loss: 3.6191 - acc: 0.3800\n",
            "98/98 [==============================] - 98s 1s/step - loss: 3.3182 - acc: 0.4458 - val_loss: 3.6200 - val_acc: 0.3800\n",
            "Epoch 22/60\n",
            "10000/10000 [==============================] - 4s 364us/sample - loss: 4.1751 - acc: 0.2826\n",
            "98/98 [==============================] - 98s 1s/step - loss: 3.3672 - acc: 0.4265 - val_loss: 4.1756 - val_acc: 0.2826\n",
            "Epoch 23/60\n",
            "10000/10000 [==============================] - 4s 373us/sample - loss: 4.9653 - acc: 0.1758\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.4275 - acc: 0.4064 - val_loss: 4.9648 - val_acc: 0.1758\n",
            "Epoch 24/60\n",
            "10000/10000 [==============================] - 4s 365us/sample - loss: 4.6927 - acc: 0.2206\n",
            "98/98 [==============================] - 100s 1s/step - loss: 3.4471 - acc: 0.3944 - val_loss: 4.6918 - val_acc: 0.2206\n",
            "Epoch 25/60\n",
            "10000/10000 [==============================] - 4s 362us/sample - loss: 5.7301 - acc: 0.1481\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.4159 - acc: 0.3930 - val_loss: 5.7319 - val_acc: 0.1481\n",
            "Epoch 26/60\n",
            "10000/10000 [==============================] - 4s 374us/sample - loss: 4.9859 - acc: 0.1878\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.3978 - acc: 0.3866 - val_loss: 4.9858 - val_acc: 0.1878\n",
            "Epoch 27/60\n",
            "10000/10000 [==============================] - 4s 372us/sample - loss: 4.8219 - acc: 0.2041\n",
            "98/98 [==============================] - 100s 1s/step - loss: 3.3409 - acc: 0.3913 - val_loss: 4.8218 - val_acc: 0.2041\n",
            "Epoch 28/60\n",
            "10000/10000 [==============================] - 4s 371us/sample - loss: 4.0773 - acc: 0.2686\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.2867 - acc: 0.3945 - val_loss: 4.0761 - val_acc: 0.2686\n",
            "Epoch 29/60\n",
            "10000/10000 [==============================] - 4s 367us/sample - loss: 7.0913 - acc: 0.0895\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.2268 - acc: 0.4004 - val_loss: 7.0886 - val_acc: 0.0895\n",
            "Epoch 30/60\n",
            "10000/10000 [==============================] - 4s 358us/sample - loss: 5.0573 - acc: 0.1792\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.1495 - acc: 0.4060 - val_loss: 5.0604 - val_acc: 0.1792\n",
            "Epoch 31/60\n",
            "10000/10000 [==============================] - 4s 361us/sample - loss: 4.6607 - acc: 0.2077\n",
            "98/98 [==============================] - 99s 1s/step - loss: 3.0161 - acc: 0.4300 - val_loss: 4.6598 - val_acc: 0.2077\n",
            "Epoch 32/60\n",
            "10000/10000 [==============================] - 4s 370us/sample - loss: 3.9480 - acc: 0.2728\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.8351 - acc: 0.4616 - val_loss: 3.9499 - val_acc: 0.2728\n",
            "Epoch 33/60\n",
            "10000/10000 [==============================] - 4s 364us/sample - loss: 4.2914 - acc: 0.2342\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.6891 - acc: 0.4870 - val_loss: 4.2927 - val_acc: 0.2342\n",
            "Epoch 34/60\n",
            "10000/10000 [==============================] - 4s 362us/sample - loss: 3.0719 - acc: 0.4127\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.5473 - acc: 0.5112 - val_loss: 3.0719 - val_acc: 0.4127\n",
            "Epoch 35/60\n",
            "10000/10000 [==============================] - 4s 353us/sample - loss: 3.2061 - acc: 0.3917\n",
            "98/98 [==============================] - 98s 1s/step - loss: 2.4246 - acc: 0.5341 - val_loss: 3.2057 - val_acc: 0.3917\n",
            "Epoch 36/60\n",
            "10000/10000 [==============================] - 4s 446us/sample - loss: 3.1157 - acc: 0.4026\n",
            "98/98 [==============================] - 100s 1s/step - loss: 2.2959 - acc: 0.5592 - val_loss: 3.1161 - val_acc: 0.4026\n",
            "Epoch 37/60\n",
            "10000/10000 [==============================] - 4s 361us/sample - loss: 2.8135 - acc: 0.4504\n",
            "98/98 [==============================] - 98s 1s/step - loss: 2.1793 - acc: 0.5800 - val_loss: 2.8143 - val_acc: 0.4504\n",
            "Epoch 38/60\n",
            "10000/10000 [==============================] - 4s 360us/sample - loss: 2.7479 - acc: 0.4685\n",
            "98/98 [==============================] - 98s 1s/step - loss: 2.0579 - acc: 0.6057 - val_loss: 2.7479 - val_acc: 0.4685\n",
            "Epoch 39/60\n",
            "10000/10000 [==============================] - 4s 375us/sample - loss: 2.5266 - acc: 0.5107\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.9324 - acc: 0.6342 - val_loss: 2.5268 - val_acc: 0.5107\n",
            "Epoch 40/60\n",
            "10000/10000 [==============================] - 4s 369us/sample - loss: 2.4341 - acc: 0.5271\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.8127 - acc: 0.6617 - val_loss: 2.4345 - val_acc: 0.5271\n",
            "Epoch 41/60\n",
            "10000/10000 [==============================] - 4s 358us/sample - loss: 2.4730 - acc: 0.5156\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.7649 - acc: 0.6726 - val_loss: 2.4734 - val_acc: 0.5156\n",
            "Epoch 42/60\n",
            "10000/10000 [==============================] - 4s 372us/sample - loss: 2.8259 - acc: 0.4536\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.8410 - acc: 0.6452 - val_loss: 2.8265 - val_acc: 0.4536\n",
            "Epoch 43/60\n",
            "10000/10000 [==============================] - 4s 370us/sample - loss: 2.8982 - acc: 0.4417\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.9565 - acc: 0.6157 - val_loss: 2.8981 - val_acc: 0.4417\n",
            "Epoch 44/60\n",
            "10000/10000 [==============================] - 4s 360us/sample - loss: 4.0391 - acc: 0.2923\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.0944 - acc: 0.5797 - val_loss: 4.0410 - val_acc: 0.2923\n",
            "Epoch 45/60\n",
            "10000/10000 [==============================] - 4s 371us/sample - loss: 4.0668 - acc: 0.2933\n",
            "98/98 [==============================] - 100s 1s/step - loss: 2.2136 - acc: 0.5556 - val_loss: 4.0649 - val_acc: 0.2933\n",
            "Epoch 46/60\n",
            "10000/10000 [==============================] - 4s 363us/sample - loss: 5.0743 - acc: 0.2256\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.3112 - acc: 0.5353 - val_loss: 5.0751 - val_acc: 0.2256\n",
            "Epoch 47/60\n",
            "10000/10000 [==============================] - 4s 363us/sample - loss: 5.1770 - acc: 0.1500\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.3610 - acc: 0.5264 - val_loss: 5.1758 - val_acc: 0.1500\n",
            "Epoch 48/60\n",
            "10000/10000 [==============================] - 4s 429us/sample - loss: 4.7447 - acc: 0.1982\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.4466 - acc: 0.5125 - val_loss: 4.7444 - val_acc: 0.1982\n",
            "Epoch 49/60\n",
            "10000/10000 [==============================] - 4s 363us/sample - loss: 3.9589 - acc: 0.2993\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.4725 - acc: 0.5112 - val_loss: 3.9587 - val_acc: 0.2993\n",
            "Epoch 50/60\n",
            "10000/10000 [==============================] - 4s 368us/sample - loss: 4.4058 - acc: 0.2455\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.5154 - acc: 0.5042 - val_loss: 4.4047 - val_acc: 0.2455\n",
            "Epoch 51/60\n",
            "10000/10000 [==============================] - 4s 365us/sample - loss: 4.0286 - acc: 0.3056\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.4367 - acc: 0.5263 - val_loss: 4.0306 - val_acc: 0.3056\n",
            "Epoch 52/60\n",
            "10000/10000 [==============================] - 4s 361us/sample - loss: 3.3937 - acc: 0.3722\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.2887 - acc: 0.5613 - val_loss: 3.3942 - val_acc: 0.3722\n",
            "Epoch 53/60\n",
            "10000/10000 [==============================] - 4s 378us/sample - loss: 3.0430 - acc: 0.4252\n",
            "98/98 [==============================] - 99s 1s/step - loss: 2.1761 - acc: 0.5862 - val_loss: 3.0441 - val_acc: 0.4252\n",
            "Epoch 54/60\n",
            "10000/10000 [==============================] - 4s 359us/sample - loss: 3.4520 - acc: 0.3681\n",
            "98/98 [==============================] - 98s 1s/step - loss: 2.0548 - acc: 0.6129 - val_loss: 3.4509 - val_acc: 0.3681\n",
            "Epoch 55/60\n",
            "10000/10000 [==============================] - 4s 368us/sample - loss: 2.9528 - acc: 0.4486\n",
            "98/98 [==============================] - 100s 1s/step - loss: 1.9473 - acc: 0.6370 - val_loss: 2.9541 - val_acc: 0.4486\n",
            "Epoch 56/60\n",
            "10000/10000 [==============================] - 3s 345us/sample - loss: 2.7400 - acc: 0.4820\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.8319 - acc: 0.6596 - val_loss: 2.7393 - val_acc: 0.4820\n",
            "Epoch 57/60\n",
            "10000/10000 [==============================] - 4s 354us/sample - loss: 2.5970 - acc: 0.5154\n",
            "98/98 [==============================] - 98s 1s/step - loss: 1.7130 - acc: 0.6870 - val_loss: 2.5979 - val_acc: 0.5154\n",
            "Epoch 58/60\n",
            "10000/10000 [==============================] - 4s 376us/sample - loss: 2.4772 - acc: 0.5337\n",
            "98/98 [==============================] - 99s 1s/step - loss: 1.5963 - acc: 0.7145 - val_loss: 2.4786 - val_acc: 0.5337\n",
            "Epoch 59/60\n",
            "10000/10000 [==============================] - 4s 361us/sample - loss: 2.4825 - acc: 0.5328\n",
            "98/98 [==============================] - 98s 1s/step - loss: 1.4705 - acc: 0.7443 - val_loss: 2.4843 - val_acc: 0.5328\n",
            "Epoch 60/60\n",
            "10000/10000 [==============================] - 4s 357us/sample - loss: 2.3605 - acc: 0.5528\n",
            "98/98 [==============================] - 98s 1000ms/step - loss: 1.3558 - acc: 0.7756 - val_loss: 2.3624 - val_acc: 0.5528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F8SLhVT4AAA",
        "colab_type": "code",
        "outputId": "8513a6e0-4127-4426-9e6a-cd62e5822ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Plot of accuracy and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history1.history['acc']\n",
        "val_acc = history1.history['val_acc']\n",
        "\n",
        "plt.plot(range(len(acc)), acc, color=\"blue\")\n",
        "plt.plot(range(len(val_acc)), val_acc, color=\"green\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdzK05KX6WSX",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In 60 epochs, the model obtained a maximum validation accuracy of **55.28%**. The training accuracy however, is much higher than the validation accuracy - **77.56%**. This shows that there is significant overfitting. This can be addressed by:\n",
        "1. Increasing the L2 regularization on the layer weights.\n",
        "2. Using more aggressive image augmentation.\n",
        "3. Performing negative hard mining to increase the weightage of classes the neural network is having difficulties predicting."
      ]
    }
  ]
}